{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6691f330",
   "metadata": {},
   "source": [
    "# Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25973b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFDecisionTree:\n",
    "    def __init__(self, max_depth=12, min_samples_split=20, min_samples_leaf=5, feature_indices=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.feature_indices = feature_indices\n",
    "        self.root = None\n",
    "    \n",
    "    def is_leaf_node(self, node):\n",
    "        return not isinstance(node, dict)\n",
    "\n",
    "    def gini_impurity(self, y):\n",
    "        \"\"\"Use Gini instead of Entropy for speed\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "\n",
    "    def gini_gain(self, y, feature_vector, threshold):\n",
    "        \"\"\"Faster than entropy calculation\"\"\"\n",
    "        left = feature_vector <= threshold\n",
    "        right = feature_vector > threshold\n",
    "        \n",
    "        if np.sum(left) == 0 or np.sum(right) == 0:\n",
    "            return -1\n",
    "            \n",
    "        n_left, n_right = np.sum(left), np.sum(right)\n",
    "        n_total = len(y)\n",
    "        \n",
    "        gini_left = self.gini_impurity(y[left])\n",
    "        gini_right = self.gini_impurity(y[right])\n",
    "        current_gini = self.gini_impurity(y)\n",
    "        \n",
    "        gain = current_gini - (n_left/n_total * gini_left + n_right/n_total * gini_right)\n",
    "        return gain\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"Optimized split finding with fewer threshold candidates\"\"\"\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        if self.feature_indices is not None:\n",
    "            features = self.feature_indices\n",
    "        else:\n",
    "            features = range(X.shape[1])\n",
    "            \n",
    "        for feature in features:\n",
    "            feature_values = X[:, feature]\n",
    "            \n",
    "            # Use percentiles instead of all unique values for speed\n",
    "            if len(feature_values) > 100:\n",
    "                thresholds = np.percentile(feature_values, [25, 50, 75])\n",
    "            else:\n",
    "                unique_values = np.unique(feature_values)\n",
    "                if len(unique_values) > 10:\n",
    "                    thresholds = np.percentile(unique_values, [33, 66])\n",
    "                else:\n",
    "                    thresholds = unique_values[:-1]  # All except last\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                gain = self.gini_gain(y, feature_values, threshold)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Early stopping conditions\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            len(np.unique(y)) == 1 or\n",
    "            self.gini_impurity(y) < 0.01):  # Stop if already pure\n",
    "            return self.most_common_label(y)\n",
    "        \n",
    "        feature, threshold, gain = self.best_split(X, y)\n",
    "        \n",
    "        if gain <= 0 or feature is None:  # No improvement\n",
    "            return self.most_common_label(y)\n",
    "\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        # Check minimum samples in leaves\n",
    "        if (np.sum(left_mask) < self.min_samples_leaf or \n",
    "            np.sum(right_mask) < self.min_samples_leaf):\n",
    "            return self.most_common_label(y)\n",
    "        \n",
    "        left_subtree = self.build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self.build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return {\n",
    "            'feature_index': feature, \n",
    "            'threshold': threshold, \n",
    "            'left': left_subtree, \n",
    "            'right': right_subtree\n",
    "        }\n",
    "\n",
    "    def most_common_label(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict_single(self, x, node):\n",
    "        if self.is_leaf_node(node):\n",
    "            return node\n",
    "            \n",
    "        if x[node['feature_index']] <= node['threshold']:\n",
    "            return self.predict_single(x, node['left'])\n",
    "        else:\n",
    "            return self.predict_single(x, node['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_single(x, self.root) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1516e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=50, max_depth=12, min_samples_split=20, min_samples_leaf=5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.trees = []\n",
    "        self.n_classes = None\n",
    "\n",
    "    def bootstrap_sample(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        \n",
    "        self.n_feature_samples = int(np.sqrt(n_features))\n",
    "        \n",
    "        self.trees = []\n",
    "        \n",
    "        print(f\"Training Fast Random Forest with {self.n_estimators} trees...\")\n",
    "        print(f\"Features per tree: {self.n_feature_samples}, Max depth: {self.max_depth}\")\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap sampling\n",
    "            X_bootstrap, y_bootstrap = self.bootstrap_sample(X, y)\n",
    "            \n",
    "            # Feature sampling\n",
    "            feature_indices = np.random.choice(n_features, self.n_feature_samples, replace=False)\n",
    "\n",
    "            # Create and train tree with optimized parameters\n",
    "            tree = RFDecisionTree(\n",
    "                max_depth=self.max_depth, \n",
    "                min_samples_split=self.min_samples_split, \n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                feature_indices=feature_indices\n",
    "            )\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Progress tracking\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Trained tree {i + 1}/{self.n_estimators}\")\n",
    "        \n",
    "        print(\"Random Forest training completed!\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):        \n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Batch prediction for efficiency\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "        # Majority voting\n",
    "        final_predictions = []\n",
    "        for sample_idx in range(X.shape[0]):\n",
    "            sample_predictions = tree_predictions[:, sample_idx]\n",
    "            majority_vote = Counter(sample_predictions).most_common(1)[0][0]\n",
    "            final_predictions.append(majority_vote)\n",
    "        \n",
    "        return np.array(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER CONFIGURATIONS FOR 5-MINUTE TRAINING:\n",
    "\n",
    "class VeryFastRF(RandomForestClassifier):\n",
    "    \"\"\"Fastest training (~2-3 minutes) - Good for testing\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            n_estimators=30,           # Fewer trees\n",
    "            max_depth=10,              # Shallower trees\n",
    "            min_samples_split=30,      # Prevent over-splitting\n",
    "            min_samples_leaf=10,       # Larger leaves\n",
    "            max_features='sqrt'        # Feature sampling\n",
    "        )\n",
    "\n",
    "class BalancedRF(RandomForestClassifier):\n",
    "    \"\"\"Best balance (~3-4 minutes) - RECOMMENDED\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            n_estimators=50,\n",
    "            max_depth=12,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=5,\n",
    "            max_features='sqrt'\n",
    "        )\n",
    "\n",
    "class AccurateRF(RandomForestClassifier):\n",
    "    \"\"\"Higher accuracy (~4-5 minutes)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            n_estimators=70,\n",
    "            max_depth=15,\n",
    "            min_samples_split=15,\n",
    "            min_samples_leaf=3,\n",
    "            max_features='sqrt'\n",
    "        )\n",
    "\n",
    "\n",
    "# USAGE EXAMPLES:\n",
    "def train_fast_random_forest():\n",
    "    # Load your data\n",
    "    # Xtrain, ytrain, Xval, yval = read_data(...)\n",
    "    \n",
    "    print(f\"Training data shape: {Xtrain.shape}\")\n",
    "    \n",
    "    # Choose based on your time constraints\n",
    "    rf_model = BalancedRF()  # Recommended\n",
    "    \n",
    "    # Train\n",
    "    rf_model.fit(Xtrain, ytrain)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_pred = rf_model.predict(Xtrain)\n",
    "    val_pred = rf_model.predict(Xval)\n",
    "    \n",
    "    train_acc = np.mean(train_pred == ytrain)\n",
    "    val_acc = np.mean(val_pred == yval)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with different configurations:\n",
    "# print(\"Testing VeryFastRF (2-3 minutes):\")\n",
    "# model1 = VeryFastRF()\n",
    "# model1.fit(Xtrain, ytrain)\n",
    "\n",
    "# print(\"\\nTesting BalancedRF (3-4 minutes):\")\n",
    "# model2 = BalancedRF()\n",
    "# model2.fit(Xtrain, ytrain)\n",
    "\n",
    "# print(\"\\nTesting AccurateRF (4-5 minutes):\")\n",
    "# model3 = AccurateRF()\n",
    "# model3.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbeba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [model1, model2, model3]:\n",
    "#     ypred = i.predict(Xval)\n",
    "#     print(accuracy_score(yval, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d3298",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=10, max_iter=100, tol=1e-4, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.centroids = None\n",
    "        self.labels_ = None\n",
    "        self.cluster_to_label = None\n",
    "\n",
    "    def _compute_distances(self, X, centroids):\n",
    "        return np.sqrt(((X[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.array(X, dtype=float)\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize centroids\n",
    "        random_idxs = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
    "        self.centroids = X[random_idxs]\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            distances = self._compute_distances(X, self.centroids)\n",
    "            labels = np.argmin(distances, axis=1)\n",
    "            new_centroids = np.zeros((self.n_clusters, n_features))\n",
    "            for i in range(self.n_clusters):\n",
    "                members = X[labels == i]\n",
    "                new_centroids[i] = members.mean(axis=0) if len(members) > 0 else self.centroids[i]\n",
    "            shift = np.linalg.norm(self.centroids - new_centroids)\n",
    "            self.centroids = new_centroids\n",
    "            if shift < self.tol:\n",
    "                break\n",
    "\n",
    "        self.labels_ = np.argmin(self._compute_distances(X, self.centroids), axis=1)\n",
    "\n",
    "        if y is not None:\n",
    "            self.cluster_to_label = {}\n",
    "            for c in range(self.n_clusters):\n",
    "                members = y[self.labels_ == c]\n",
    "                self.cluster_to_label[c] = Counter(members).most_common(1)[0][0] if len(members) > 0 else -1\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X, dtype=float)\n",
    "        distances = self._compute_distances(X, self.centroids)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def predict_labels(self, X):\n",
    "        if self.cluster_to_label is None:\n",
    "            raise ValueError(\"Fit with y first to assign labels.\")\n",
    "        clusters = self.predict(X)\n",
    "        return np.array([self.cluster_to_label[c] for c in clusters])\n",
    "    \n",
    "    def cluster_confidences(kmeans, y_true):\n",
    "        confidences = {}\n",
    "        for c in range(kmeans.n_clusters):\n",
    "            members = y_true[kmeans.labels_ == c]\n",
    "            if len(members) == 0:\n",
    "                confidences[c] = 0\n",
    "            else:\n",
    "                majority_label, count = Counter(members).most_common(1)[0]\n",
    "                confidences[c] = count / len(members)\n",
    "        return confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8d19a",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, value=None, left=None, right=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostMultiClass:\n",
    "    def __init__(self, n_estimators=50, learning_rate=0.1, max_depth=6, lambda_l2=1.0, gamma=0.1, \n",
    "                 min_child_weight=5, subsample=0.8, colsample_bytree=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_l2 = lambda_l2\n",
    "        self.gamma = gamma\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.trees = []  # List of lists: [class][tree]\n",
    "        self.initial_predictions = None\n",
    "        self.classes = None\n",
    "        self.n_features = None\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_gradients(self, y_true, scores):\n",
    "        \"\"\"Compute gradients and hessians for multi-class classification\"\"\"\n",
    "        n_samples, n_classes = scores.shape\n",
    "        probs = self.softmax(scores)\n",
    "        \n",
    "        y_one_hot = np.eye(n_classes)[y_true]\n",
    "        gradients = probs - y_one_hot\n",
    "        hessians = probs * (1 - probs)\n",
    "        \n",
    "        return gradients, hessians\n",
    "\n",
    "    def gain(self, G, H, G_left, H_left, G_right, H_right):\n",
    "        \"\"\"Calculate split gain using XGBoost formula\"\"\"\n",
    "        def calc_term(G, H):\n",
    "            return (G ** 2) / (H + self.lambda_l2)\n",
    "        \n",
    "        gain = 0.5 * (calc_term(G_left, H_left) + calc_term(G_right, H_right) - calc_term(G, H)) - self.gamma\n",
    "        return gain\n",
    "\n",
    "    def best_split(self, X, gradients, hessians, feature):\n",
    "        \"\"\"Find best split for a single feature\"\"\"\n",
    "        feature_values = X[:, feature]\n",
    "        \n",
    "        if len(feature_values) > 1000:\n",
    "            thresholds = np.percentile(feature_values, [25, 50, 75])\n",
    "        else:\n",
    "            unique_vals = np.unique(feature_values)\n",
    "            if len(unique_vals) > 10:\n",
    "                thresholds = np.percentile(unique_vals, [20, 40, 60, 80])\n",
    "            else:\n",
    "                thresholds = unique_vals[:-1]\n",
    "        \n",
    "        best_gain = 0\n",
    "        best_threshold = None\n",
    "        \n",
    "        G = np.sum(gradients)\n",
    "        H = np.sum(hessians)\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            left_mask = feature_values <= threshold\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                continue\n",
    "                \n",
    "            G_left = np.sum(gradients[left_mask])\n",
    "            H_left = np.sum(hessians[left_mask])\n",
    "            G_right = G - G_left\n",
    "            H_right = H - H_left\n",
    "\n",
    "            if H_left < self.min_child_weight or H_right < self.min_child_weight:\n",
    "                continue\n",
    "            \n",
    "            current_gain = self.gain(G, H, G_left, H_left, G_right, H_right)\n",
    "            \n",
    "            if current_gain > best_gain:\n",
    "                best_gain = current_gain\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        return best_gain, best_threshold\n",
    "\n",
    "    def build_tree(self, X, gradients, hessians, depth=0):\n",
    "        \"\"\"Build a single regression tree\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Stopping conditions\n",
    "        if (depth >= self.max_depth or n_samples < 2 or np.sum(hessians) < self.min_child_weight):\n",
    "            leaf_value = -np.sum(gradients) / (np.sum(hessians) + self.lambda_l2)\n",
    "            return TreeNode(value=leaf_value)\n",
    "        \n",
    "        best_gain = 0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        # Data subsampling\n",
    "        if self.subsample < 1.0:\n",
    "            subsample_size = int(self.subsample * n_samples)\n",
    "            rows = np.random.choice(n_samples, subsample_size, replace=False)\n",
    "            X, gradients, hessians = X[rows], gradients[rows], hessians[rows]\n",
    "            n_samples = subsample_size\n",
    "\n",
    "        features = range(n_features)\n",
    "\n",
    "        for feature in features:\n",
    "            gain, threshold = self.best_split(X, gradients, hessians, feature)\n",
    "            \n",
    "            if gain > best_gain and threshold is not None:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        if best_gain == 0:\n",
    "            leaf_value = -np.sum(gradients) / (np.sum(hessians) + self.lambda_l2)\n",
    "            return TreeNode(value=leaf_value)\n",
    "        \n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        left_node = self.build_tree(X[left_mask], gradients[left_mask], hessians[left_mask], depth + 1)\n",
    "        right_node = self.build_tree(X[right_mask], gradients[right_mask], hessians[right_mask], depth + 1)\n",
    "        \n",
    "        return TreeNode(feature_index=best_feature, threshold=best_threshold, left=left_node, right=right_node)\n",
    "\n",
    "    def predict_tree(self, x, node):\n",
    "        \"\"\"Predict using a single tree\"\"\"\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if node.feature_index < len(x) and x[node.feature_index] <= node.threshold:\n",
    "            return self.predict_tree(x, node.left)\n",
    "        else:\n",
    "            return self.predict_tree(x, node.right)\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"Train the XGBoost model without early stopping\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # --- Make sure labels start at 0 ---\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        n_samples, self.n_features = X.shape\n",
    "\n",
    "        # Remap y to 0..n_classes-1\n",
    "        class_to_index = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        y_mapped = np.array([class_to_index[val] for val in y])\n",
    "\n",
    "        print(f\"Training XGBoost on {n_samples} samples, {self.n_features} features, {n_classes} classes\")\n",
    "\n",
    "        # Initialize class probabilities\n",
    "        class_counts = np.bincount(y_mapped, minlength=n_classes)\n",
    "        class_probs = np.clip(class_counts / n_samples, 1e-15, 1 - 1e-15)\n",
    "        self.initial_predictions = np.log(class_probs)\n",
    "\n",
    "        # Initialize scores for training\n",
    "        train_scores = np.tile(self.initial_predictions, (n_samples, 1))\n",
    "\n",
    "        self.trees = [[] for _ in range(n_classes)]\n",
    "\n",
    "        for iteration in range(self.n_estimators):\n",
    "            gradients, hessians = self.compute_gradients(y_mapped, train_scores)\n",
    "\n",
    "            # Train trees for each class\n",
    "            for class_idx in range(n_classes):\n",
    "                tree = self.build_tree(X, gradients[:, class_idx], hessians[:, class_idx])\n",
    "                self.trees[class_idx].append(tree)\n",
    "\n",
    "                # Update training scores\n",
    "                tree_pred_train = np.array([self.predict_tree(x, tree) for x in X])\n",
    "                train_scores[:, class_idx] += self.learning_rate * tree_pred_train\n",
    "\n",
    "            # Print progress\n",
    "            train_probs = self.softmax(train_scores)\n",
    "            train_pred = np.argmax(train_probs, axis=1)\n",
    "            train_acc = np.mean(train_pred == y_mapped)\n",
    "\n",
    "            if X_val is not None and y_val is not None and iteration % 5 == 0:\n",
    "                val_pred = self.predict(X_val)\n",
    "                val_acc = np.mean(val_pred == y_val)\n",
    "                print(f\"Iteration {iteration}: Train acc = {train_acc:.4f}, Val acc = {val_acc:.4f}\")\n",
    "            else:\n",
    "                print(f\"Iteration {iteration}: Train accuracy = {train_acc:.4f}\")\n",
    "\n",
    "        print(f\"Training completed with {self.n_estimators} iterations\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes)\n",
    "        scores = np.full((n_samples, n_classes), self.initial_predictions)\n",
    "\n",
    "        for class_idx in range(n_classes):\n",
    "            for tree in self.trees[class_idx]:\n",
    "                tree_pred = np.array([self.predict_tree(x, tree) for x in X])\n",
    "                scores[:, class_idx] += self.learning_rate * tree_pred\n",
    "\n",
    "        return self.softmax(scores)\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        class_indices = np.argmax(proba, axis=1)\n",
    "        return self.classes[class_indices]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072405b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fixed version\n",
    "# print(\"Testing XGBoost:\")\n",
    "# model = XGBoostMultiClass(\n",
    "#     n_estimators=50,\n",
    "#     learning_rate=0.15,\n",
    "#     max_depth=4,\n",
    "#     lambda_l2=0.5,\n",
    "#     subsample=0.8\n",
    "# )\n",
    "\n",
    "# model.fit(Xtrain_pca, ytrain, Xval_pca, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e800558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ypred = model.predict(Xtest_pca)\n",
    "# print(f1_score(ytest, ypred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c750679a",
   "metadata": {},
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb79bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_epochs=100, batch_size=100, lambda_reg=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lambda_reg = lambda_reg  # L2 regularization\n",
    "        \n",
    "        self.theta = None\n",
    "        self.classes = None\n",
    "        self.n_features = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"Add bias column to feature matrix\"\"\"\n",
    "        if X.ndim == 1:\n",
    "            X = X[:, np.newaxis]\n",
    "        return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        \"\"\"Softmax with numerical stability\"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def _compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Cross-entropy loss with L2 regularization\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        cross_entropy = -np.sum(y_true * np.log(y_pred + 1e-15)) / m\n",
    "        reg = (self.lambda_reg / (2 * m)) * np.sum(self.theta[1:] ** 2)\n",
    "        return cross_entropy + reg\n",
    "\n",
    "    def _one_hot_encode(self, y, n_classes):\n",
    "        \"\"\"Convert integer labels to one-hot encoding\"\"\"\n",
    "        y_one_hot = np.zeros((len(y), n_classes))\n",
    "        y_one_hot[np.arange(len(y)), y] = 1\n",
    "        return y_one_hot\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the softmax regression model\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        X_b = self._add_bias(X)\n",
    "        m, n = X_b.shape\n",
    "        y_one_hot = self._one_hot_encode(y, n_classes)\n",
    "\n",
    "        self.theta = np.random.randn(n, n_classes) * 0.01\n",
    "        n_batches = max(1, m // self.batch_size)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X_b[indices]\n",
    "            y_shuffled = y_one_hot[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * self.batch_size\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                logits = np.dot(X_batch, self.theta)\n",
    "                y_pred = self._softmax(logits)\n",
    "\n",
    "                errors = y_pred - y_batch\n",
    "                gradients = (1.0 / len(X_batch)) * np.dot(X_batch.T, errors)\n",
    "                gradients[1:] += (self.lambda_reg / m) * self.theta[1:]\n",
    "\n",
    "                self.theta -= self.learning_rate * gradients\n",
    "                epoch_loss += self._compute_loss(y_batch, y_pred)\n",
    "\n",
    "            self.loss_history.append(epoch_loss / n_batches)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        X_b = self._add_bias(np.asarray(X))\n",
    "        logits = np.dot(X_b, self.theta)\n",
    "        return self._softmax(logits)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"Compute accuracy\"\"\"\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(preds == y)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"Return model parameters\"\"\"\n",
    "        return {\n",
    "            'theta': self.theta,\n",
    "            'classes': self.classes,\n",
    "            'n_features': self.n_features,\n",
    "            'loss_history': self.loss_history\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax = SoftmaxRegression(\n",
    "#             learning_rate=0.05,\n",
    "#             n_epochs=150,\n",
    "#             batch_size=50,\n",
    "#             lambda_reg=0.01,\n",
    "#             verbose=True\n",
    "#         )\n",
    "# softmax.fit(Xtrain, ytrain)\n",
    "# ypred = softmax.predict(Xtest)\n",
    "# print(accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc07905",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import albumentations as A\n",
    "\n",
    "# Albumentations uses (H,W,C), so add a channel dimension later\n",
    "augmenter = A.Compose([\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.Affine(shear=10, p=0.4),\n",
    "    A.Perspective(scale=(0.02, 0.05), p=0.4),\n",
    "    A.GaussNoise(var_limit=(5.0, 20.0), p=0.3)\n",
    "])\n",
    "\n",
    "def augment_data(X, y, multiplier=2):\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    X_aug_list = []\n",
    "    y_aug_list = []\n",
    "\n",
    "    X_reshaped = X.reshape(-1, 28, 28)\n",
    "\n",
    "    for _ in range(multiplier):\n",
    "        images_aug = []\n",
    "        for img in X_reshaped:\n",
    "            aug = augmenter(image=img.astype(np.uint8))\n",
    "            images_aug.append(aug[\"image\"])\n",
    "        X_aug_list.append(np.array(images_aug).reshape(len(X), -1))\n",
    "        y_aug_list.append(y)\n",
    "\n",
    "    X_final = np.vstack([X] + X_aug_list)\n",
    "    y_final = np.hstack([y] + y_aug_list)\n",
    "    return X_final, y_final\n",
    "\n",
    "# Example:\n",
    "# Xtrain_aug, ytrain_aug = augment_data(Xtrain, ytrain, multiplier=2)\n",
    "# print(\"Augmented:\", Xtrain_aug.shape, ytrain_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca1 = PCA(n_components=120)\n",
    "# Xtrain_aug_pca = pca1.fit_transform(Xtrain_aug)\n",
    "# Xval_aug_pca = pca1.transform(Xval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
