{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05ef047",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abeca317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c710f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(trainfile='MNIST_train.csv', validationfile='MNIST_validation.csv', testfile='MNIST_test.csv'):\n",
    "    \n",
    "    dftrain = pd.read_csv(trainfile)\n",
    "    dfval = pd.read_csv(validationfile)\n",
    "    dftest = pd.read_csv(testfile)\n",
    "\n",
    "    featurecols = list(dftrain.columns)\n",
    "    featurecols.remove('label')\n",
    "    featurecols.remove('even')\n",
    "\n",
    "    targetcol1 = 'label'\n",
    "    targetcol2 = 'even'\n",
    "\n",
    "    Xtrain = dftrain[featurecols]\n",
    "    ytrain = dftrain[targetcol1]\n",
    "    ytrain2 = dftrain[targetcol2]\n",
    "    \n",
    "    Xval = dfval[featurecols]\n",
    "    yval = dfval[targetcol1]\n",
    "    yval2 = dfval[targetcol2]\n",
    "\n",
    "    Xtest = dftest[featurecols]\n",
    "    ytest = dftest[targetcol1]\n",
    "\n",
    "    return (Xtrain, Xval, Xtest, ytrain, yval, ytest, ytrain2, yval2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b84c1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xval, Xtest, ytrain, yval, ytest, ytrain2, yval2 = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5042931",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66aae01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    \"\"\"PCA class that can be fitted once and reused\"\"\"\n",
    "    def __init__(self, variance_threshold=0.95, n_components=None):\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        self.explained_variance_ratio = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit PCA on training data only\"\"\"\n",
    "        # Normalize and center data\n",
    "        X_normalized = np.array(X, dtype=float) / 255.0\n",
    "        self.mean = np.mean(X_normalized, axis=0)\n",
    "        X_centered = X_normalized - self.mean\n",
    "        \n",
    "        # Compute covariance matrix and eigen decomposition\n",
    "        evd_matrix = (X_centered.T @ X_centered) / (X_centered.shape[0] - 1)\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(evd_matrix)\n",
    "        \n",
    "        # Sort by descending eigenvalues\n",
    "        sorted_idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues_sorted = eigenvalues[sorted_idx]\n",
    "        eigenvectors_sorted = eigenvectors[:, sorted_idx]\n",
    "        \n",
    "        # Determine components for target variance\n",
    "        self.explained_variance_ratio = eigenvalues_sorted / np.sum(eigenvalues_sorted)\n",
    "        cumulative_variance = np.cumsum(self.explained_variance_ratio)\n",
    "\n",
    "        if self.n_components is None:\n",
    "            self.n_components = np.argmax(cumulative_variance >= self.variance_threshold) + 1\n",
    "        \n",
    "        # Store components\n",
    "        self.components = eigenvectors_sorted[:, :self.n_components]\n",
    "        \n",
    "        print(f\"PCA fitted with {self.n_components} components, explaining {cumulative_variance[self.n_components-1]:.4f} variance\")\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data using fitted PCA\"\"\"\n",
    "        if self.components is None:\n",
    "            raise ValueError(\"PCA must be fitted before transforming\")\n",
    "            \n",
    "        # Apply same normalization and centering\n",
    "        X_normalized = np.array(X, dtype=float) / 255.0\n",
    "        X_centered = X_normalized - self.mean\n",
    "        \n",
    "        # Project to PCA space\n",
    "        return X_centered @ self.components\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a15f86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA fitted with 152 components, explaining 0.9501 variance\n",
      "Reduced from 784 to 152 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Transform data to low dimensions\n",
    "pca = PCA()\n",
    "Xtrain_pca = pca.fit_transform(Xtrain)\n",
    "Xval_pca = pca.transform(Xval)\n",
    "\n",
    "# Ready for k-NN classification\n",
    "print(f\"Reduced from 784 to {Xtrain_pca.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629a49e",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf8ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=5, distance_metric='euclidean', weights='uniform'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = np.asarray(X)\n",
    "        self.y_train = np.asarray(y)\n",
    "        self.classes = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def _compute_all_distances(self, X):\n",
    "        \"\"\"\n",
    "        Compute distances between X (n_test × d) and train (n_train × d)\n",
    "        Fully vectorized\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            # (x - y)^2 = x^2 + y^2 - 2xy\n",
    "            X2 = np.sum(X**2, axis=1).reshape(-1, 1)\n",
    "            T2 = np.sum(self.X_train**2, axis=1).reshape(1, -1)\n",
    "            dist = np.sqrt(np.maximum(X2 + T2 - 2 * X @ self.X_train.T, 0))\n",
    "\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            # |x - y|\n",
    "            # Use broadcasting\n",
    "            dist = np.sum(np.abs(X[:, None, :] - self.X_train[None, :, :]), axis=2)\n",
    "\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            # 1 - cosine similarity\n",
    "            X_norm = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-15)\n",
    "            T_norm = self.X_train / (np.linalg.norm(self.X_train, axis=1, keepdims=True) + 1e-15)\n",
    "            dist = 1 - X_norm @ T_norm.T\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported metric\")\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # vectorized distance computation\n",
    "        dist = self._compute_all_distances(X)\n",
    "\n",
    "        # get k nearest neighbors\n",
    "        idx = np.argpartition(dist, self.k, axis=1)[:, :self.k]\n",
    "        neighbors = self.y_train[idx]\n",
    "\n",
    "        if self.weights == 'uniform':\n",
    "            # majority vote\n",
    "            return np.array([Counter(row).most_common(1)[0][0] for row in neighbors])\n",
    "\n",
    "        else:  # distance-weighted\n",
    "            kdist = np.take_along_axis(dist, idx, axis=1)\n",
    "            weights = 1.0 / (kdist + 1e-15)\n",
    "            preds = []\n",
    "            for lbls, w in zip(neighbors, weights):\n",
    "                vote = {}\n",
    "                for label, weight in zip(lbls, w):\n",
    "                    vote[label] = vote.get(label, 0) + weight\n",
    "                preds.append(max(vote.items(), key=lambda a: a[1])[0])\n",
    "            return np.array(preds)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "        dist = self._compute_all_distances(X)\n",
    "\n",
    "        idx = np.argpartition(dist, self.k, axis=1)[:, :self.k]\n",
    "        neighbors = self.y_train[idx]\n",
    "\n",
    "        probs = np.zeros((len(X), len(self.classes)))\n",
    "\n",
    "        for i, row in enumerate(neighbors):\n",
    "            for j, cls in enumerate(self.classes):\n",
    "                probs[i, j] = np.sum(row == cls) / self.k\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb8f9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KNN(k=1)\n",
    "# model.fit(Xtrain_pca, ytrain)\n",
    "# ypred = model.predict(Xval_pca)\n",
    "# print(accuracy_score(yval, ypred))\n",
    "# print(confusion_matrix(yval, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb0a08",
   "metadata": {},
   "source": [
    "# XGBoost Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f0fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    \"\"\"Simple tree node structure\"\"\"\n",
    "    def __init__(self, feature_index=None, threshold=None, value=None, left=None, right=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "737701d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostClassifier:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3,\n",
    "                 lambda_l2=1, gamma=0, min_child_weight=1, threshold=0.5):\n",
    "        \n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_l2 = lambda_l2\n",
    "        self.gamma = gamma\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.trees = []\n",
    "        self.initial_prediction = None\n",
    "\n",
    "\n",
    "    # ================================\n",
    "    # MATH\n",
    "    # ================================\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -12, 12)))\n",
    "\n",
    "    def log_loss_derivatives(self, y_true, y_pred):\n",
    "        p = self.sigmoid(y_pred)\n",
    "        grad = p - y_true\n",
    "        hess = p * (1 - p)\n",
    "        return grad, hess\n",
    "\n",
    "    def calc_gain(self, G, H):\n",
    "        return (G * G) / (H + self.lambda_l2)\n",
    "\n",
    "    def compute_gain(self, G, H, G_left, H_left, G_right, H_right):\n",
    "        gain = 0.5 * (self.calc_gain(G_left, H_left) +\n",
    "                      self.calc_gain(G_right, H_right) -\n",
    "                      self.calc_gain(G, H)) - self.gamma\n",
    "        return gain\n",
    "\n",
    "\n",
    "    # ================================\n",
    "    # BEST SPLIT FOR ONE FEATURE\n",
    "    # ================================\n",
    "    def best_split_feature(self, X, grad, hess, feature, sorted_idx):\n",
    "        fv = X[sorted_idx, feature]\n",
    "\n",
    "        # cumulative sums\n",
    "        G_cum = np.cumsum(grad[sorted_idx])\n",
    "        H_cum = np.cumsum(hess[sorted_idx])\n",
    "\n",
    "        G_total = G_cum[-1]\n",
    "        H_total = H_cum[-1]\n",
    "\n",
    "        best_gain = 0\n",
    "        best_thr = None\n",
    "\n",
    "        for i in range(len(sorted_idx) - 1):\n",
    "\n",
    "            # skip identical values\n",
    "            if fv[i] == fv[i + 1]:\n",
    "                continue\n",
    "\n",
    "            G_left = G_cum[i]\n",
    "            H_left = H_cum[i]\n",
    "\n",
    "            G_right = G_total - G_left\n",
    "            H_right = H_total - H_left\n",
    "\n",
    "            if H_left < self.min_child_weight or H_right < self.min_child_weight:\n",
    "                continue\n",
    "\n",
    "            gain = self.compute_gain(G_total, H_total, G_left, H_left, G_right, H_right)\n",
    "\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_thr = (fv[i] + fv[i + 1]) / 2\n",
    "\n",
    "        return best_gain, best_thr\n",
    "\n",
    "\n",
    "    # ================================\n",
    "    # BEST SPLIT ACROSS ALL FEATURES\n",
    "    # ================================\n",
    "    def best_split(self, X, grad, hess):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Feature subsampling (sqrt rule)\n",
    "        feat_count = int(np.sqrt(n_features))\n",
    "        features = np.random.choice(n_features, feat_count, replace=False)\n",
    "\n",
    "        best_gain = 0\n",
    "        best_feat = None\n",
    "        best_thr = None\n",
    "\n",
    "        # Compute sorted indices LOCALLY for this node\n",
    "        local_sorted = {f: np.argsort(X[:, f]) for f in features}\n",
    "\n",
    "        for f in features:\n",
    "            gain, thr = self.best_split_feature(X, grad, hess, f, local_sorted[f])\n",
    "\n",
    "            if thr is not None and gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feat = f\n",
    "                best_thr = thr\n",
    "\n",
    "        return best_feat, best_thr\n",
    "\n",
    "\n",
    "    # ================================\n",
    "    # TREE BUILDING (RECURSIVE)\n",
    "    # ================================\n",
    "    def build_tree(self, X, grad, hess, depth):\n",
    "\n",
    "        if (depth >= self.max_depth or\n",
    "            len(X) < 2 or\n",
    "            hess.sum() < self.min_child_weight):\n",
    "\n",
    "            leaf_val = -grad.sum() / (hess.sum() + self.lambda_l2)\n",
    "            return TreeNode(value=leaf_val)\n",
    "\n",
    "        # best split\n",
    "        feat, thr = self.best_split(X, grad, hess)\n",
    "\n",
    "        if feat is None:\n",
    "            leaf_val = -grad.sum() / (hess.sum() + self.lambda_l2)\n",
    "            return TreeNode(value=leaf_val)\n",
    "\n",
    "        left_mask = (X[:, feat] <= thr)\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left = self.build_tree(X[left_mask], grad[left_mask], hess[left_mask], depth + 1)\n",
    "        right = self.build_tree(X[right_mask], grad[right_mask], hess[right_mask], depth + 1)\n",
    "\n",
    "        return TreeNode(feature_index=feat, threshold=thr, left=left, right=right)\n",
    "\n",
    "\n",
    "    # ================================\n",
    "    # TREE PREDICTION\n",
    "    # ================================\n",
    "    def predict_tree(self, X, node):\n",
    "        out = np.zeros(len(X))\n",
    "\n",
    "        for i, x in enumerate(X):\n",
    "            cur = node\n",
    "            while cur.value is None:\n",
    "                if x[cur.feature_index] <= cur.threshold:\n",
    "                    cur = cur.left\n",
    "                else:\n",
    "                    cur = cur.right\n",
    "            out[i] = cur.value\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    # ================================\n",
    "    # FIT\n",
    "    # ================================\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # initial prediction = log odds\n",
    "        p = np.mean(y)\n",
    "        self.initial_prediction = np.log(p / (1 - p + 1e-12))\n",
    "\n",
    "        pred = np.full(len(y), self.initial_prediction)\n",
    "\n",
    "        self.trees = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "\n",
    "            grad, hess = self.log_loss_derivatives(y, pred)\n",
    "\n",
    "            tree = self.build_tree(X, grad, hess, depth=0)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # vectorized update\n",
    "            pred += self.learning_rate * self.predict_tree(X, tree)\n",
    "\n",
    "\n",
    "    # ================================\n",
    "    # PREDICT\n",
    "    # ================================\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        pred = np.full(len(X), self.initial_prediction)\n",
    "\n",
    "        for tree in self.trees:\n",
    "            pred += self.learning_rate * self.predict_tree(X, tree)\n",
    "\n",
    "        p = self.sigmoid(pred)\n",
    "        return np.column_stack([1 - p, p])\n",
    "\n",
    "    def predict(self, X):\n",
    "        p = self.predict_proba(X)[:, 1]\n",
    "        return (p >= self.threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff166e",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cf829ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_p =0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_p = lambda_p\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert labels to -1/+1\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    # only regularization penalty\n",
    "                    self.w -= self.lr * (self.lambda_p * self.w)\n",
    "                else:\n",
    "                    # only regularization penalty\n",
    "                    self.w -= self.lr * (self.lambda_p * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b57bf",
   "metadata": {},
   "source": [
    "# Final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a13a7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hybrid_model_ovo_knn(Xtrain, ytrain, Xval, yval, ytrain_oe, yval_oe):\n",
    "\n",
    "    print(\"\\n================= PCA (GLOBAL) =================\")\n",
    "    pca_global = PCA()\n",
    "    Xtrain_pca = pca_global.fit_transform(Xtrain)\n",
    "    Xval_pca   = pca_global.transform(Xval)\n",
    "\n",
    "    print(\"\\n================= TRAIN KNN (GLOBAL) =================\")\n",
    "    knn_global = KNN(k=1)\n",
    "    knn_global.fit(Xtrain_pca, ytrain)\n",
    "    ypred_knn = knn_global.predict(Xval_pca)\n",
    "    print(\"Base KNN accuracy:\", accuracy_score(yval, ypred_knn))\n",
    "\n",
    "    print(\"\\n================= TRAIN XGB EVEN/ODD =================\")\n",
    "    model_xgb_even = XGBoostClassifier(\n",
    "        n_estimators=105,\n",
    "        learning_rate=0.3,\n",
    "        max_depth=9,\n",
    "        lambda_l2=0.1,\n",
    "        gamma=0,\n",
    "        min_child_weight=1,\n",
    "        threshold=0.5\n",
    "    )\n",
    "    model_xgb_even.fit(Xtrain, ytrain_oe)\n",
    "    ypred_xgb_even = model_xgb_even.predict(Xval)\n",
    "    print(\"XGB Even/Odd accuracy:\", accuracy_score(yval_oe, ypred_xgb_even))\n",
    "\n",
    "    print(\"\\n================= FIND PARITY MISTAKES =================\")\n",
    "    mistakes = []\n",
    "    for i in range(len(yval)):\n",
    "        if ypred_xgb_even[i] != (yval[i] % 2):\n",
    "            mistakes.append(i)\n",
    "\n",
    "    print(f\"Total parity-mismatch candidates: {len(mistakes)}\")\n",
    "\n",
    "\n",
    "    # ==================================================================\n",
    "    # ONE-VS-ONE KNN MODELS: 45 KNN MODELS + 45 PCA MODELS\n",
    "    # ==================================================================\n",
    "    print(\"\\n================= TRAINING ONE-VS-ONE KNN MODELS =================\")\n",
    "\n",
    "    ovo_knn = {}     # (digitA, digitB) -> KNN model\n",
    "    ovo_pca = {}     # (digitA, digitB) -> PCA\n",
    "\n",
    "    digits = list(range(10))\n",
    "\n",
    "    for i in digits:\n",
    "        for j in digits:\n",
    "            if i < j:\n",
    "                mask = (ytrain == i) | (ytrain == j)\n",
    "                X_pair = Xtrain[mask]\n",
    "                y_pair = ytrain[mask]\n",
    "\n",
    "                pca_pair = PCA()\n",
    "                X_pair_pca = pca_pair.fit_transform(X_pair)\n",
    "\n",
    "                knn_pair = KNN(k=1)\n",
    "                knn_pair.fit(X_pair_pca, y_pair)\n",
    "\n",
    "                ovo_knn[(i, j)] = knn_pair\n",
    "                ovo_pca[(i, j)] = pca_pair\n",
    "\n",
    "    print(\"All 45 one-vs-one KNN models trained.\")\n",
    "\n",
    "    # ==================================================================\n",
    "    # HYBRID CORRECTION USING OVO-KNN FOR MISMATCHES\n",
    "    # ==================================================================\n",
    "    print(\"\\n================= APPLY HYBRID OVO-KNN CORRECTION =================\")\n",
    "\n",
    "    y_final = ypred_knn.copy()\n",
    "\n",
    "    for idx in mistakes:\n",
    "\n",
    "        pred = ypred_knn[idx]\n",
    "        xgb_par = ypred_xgb_even[idx]   # 0=even, 1=odd\n",
    "        # true parity = yval[idx] % 2\n",
    "\n",
    "        # Step 1: only correct digits with parity conflict\n",
    "        # Step 2: pick opponent class based on XGB parity\n",
    "        if xgb_par == 0:\n",
    "            # XGB says EVEN → candidate even digits {0,2,4,6,8}\n",
    "            candidates = [0,2,4,6,8]\n",
    "        else:\n",
    "            # XGB says ODD → candidate odd digits {1,3,5,7,9}\n",
    "            candidates = [1,3,5,7,9]\n",
    "\n",
    "        # Step 3: Run pairwise comparisons: pred vs each candidate\n",
    "        votes = {}\n",
    "\n",
    "        for d in candidates:\n",
    "            if d == pred:\n",
    "                continue\n",
    "\n",
    "            a, b = min(pred, d), max(pred, d)\n",
    "\n",
    "            # Transform with this pair's PCA\n",
    "            xval_p = ovo_pca[(a, b)].transform(Xval.iloc[idx:idx+1])\n",
    "            vote = ovo_knn[(a, b)].predict(xval_p)[0]\n",
    "\n",
    "            votes[d] = vote\n",
    "\n",
    "        # Step 4: pick the most common voted label\n",
    "        if len(votes.values()) > 0:\n",
    "            best = max(votes.values(), key=list(votes.values()).count)\n",
    "            y_final[idx] = best\n",
    "\n",
    "\n",
    "    # ==================================================================\n",
    "    # RESULTS\n",
    "    # ==================================================================\n",
    "    print(\"\\n================= FINAL RESULTS =================\")\n",
    "    acc = accuracy_score(yval, y_final)\n",
    "    print(\"Final hybrid accuracy:\", acc)\n",
    "    print(confusion_matrix(yval, y_final))\n",
    "\n",
    "    return y_final, ypred_knn, ypred_xgb_even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281adde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_hybrid_model_ovo_knn(Xtrain, ytrain, Xval, yval, ytrain_oe, yval_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31203ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ovo_knn_models(Xtrain, ytrain):\n",
    "    \"\"\"\n",
    "    Trains 45 one-vs-one (i<j) PCA + KNN(k=1) models.\n",
    "    Returns:\n",
    "        ovo_knn[(i,j)] = trained KNN\n",
    "        ovo_pca[(i,j)] = trained PCA\n",
    "    \"\"\"\n",
    "    print(\"\\n================= TRAINING 45 OVO KNN MODELS =================\")\n",
    "\n",
    "    ovo_knn = {}\n",
    "    ovo_pca = {}\n",
    "\n",
    "    digits = list(range(10))\n",
    "\n",
    "    for i in digits:\n",
    "        for j in digits:\n",
    "            if i < j:\n",
    "\n",
    "                # Select samples belonging to digit i or j\n",
    "                mask = (ytrain == i) | (ytrain == j)\n",
    "                X_pair = Xtrain[mask]\n",
    "                y_pair = ytrain[mask]\n",
    "\n",
    "                # PCA for this pair\n",
    "                pca_pair = PCA(n_components=30)\n",
    "                X_pair_pca = pca_pair.fit_transform(X_pair)\n",
    "\n",
    "                # KNN for this pair\n",
    "                knn_pair = KNN(k=1)\n",
    "                knn_pair.fit(X_pair_pca, y_pair)\n",
    "\n",
    "                ovo_knn[(i, j)] = knn_pair\n",
    "                ovo_pca[(i, j)] = pca_pair\n",
    "\n",
    "                print(f\"✓ OVO model trained for pair ({i}, {j}) | Samples: {len(X_pair)}\")\n",
    "\n",
    "    print(\"\\nAll 45 OVO KNN models trained successfully.\")\n",
    "    return ovo_knn, ovo_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e98478ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ovo_knn_predict(ovo_knn, ovo_pca, X):\n",
    "    \"\"\"\n",
    "    Predict using all 45 OVO KNN models with majority voting.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    y_pred = np.zeros(n, dtype=int)\n",
    "\n",
    "    print(\"\\n================= OVO-KNN PREDICTION =================\")\n",
    "\n",
    "    for idx in range(n):\n",
    "        x = X.iloc[idx:idx+1]\n",
    "\n",
    "        votes = []\n",
    "\n",
    "        for (i, j), knn_model in ovo_knn.items():\n",
    "            pca_model = ovo_pca[(i, j)]\n",
    "\n",
    "            # project x into PCA space for this pair\n",
    "            x_pca = pca_model.transform(x)\n",
    "\n",
    "            # predict i or j\n",
    "            vote = knn_model.predict(x_pca)[0]\n",
    "            votes.append(vote)\n",
    "\n",
    "        # majority vote\n",
    "        y_pred[idx] = max(set(votes), key=votes.count)\n",
    "\n",
    "        if idx % 200 == 0:\n",
    "            print(f\"Predicted {idx}/{n}\")\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1a3cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ovo_knn_pipeline(train_csv='MNIST_train.csv', val_csv='MNIST_validation.csv'):\n",
    "\n",
    "    # Load data\n",
    "    Xtrain, ytrain, Xval, yval, ytrain_oe, yval_oe = read_data(train_csv, val_csv)\n",
    "\n",
    "    # Train 45 OVO models\n",
    "    ovo_knn, ovo_pca = train_ovo_knn_models(Xtrain, ytrain)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_pred = ovo_knn_predict(ovo_knn, ovo_pca, Xval)\n",
    "\n",
    "    # Show results\n",
    "    acc = accuracy_score(yval, y_pred)\n",
    "    cm  = confusion_matrix(yval, y_pred)\n",
    "\n",
    "    print(\"\\n================= FINAL OVO-KNN RESULTS =================\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(cm)\n",
    "\n",
    "    return y_pred, ovo_knn, ovo_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ce8d416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= TRAINING 45 OVO KNN MODELS =================\n",
      "PCA fitted with 30 components, explaining 0.8550 variance\n",
      "✓ OVO model trained for pair (0, 1) | Samples: 2111\n",
      "PCA fitted with 30 components, explaining 0.7797 variance\n",
      "✓ OVO model trained for pair (0, 2) | Samples: 1980\n",
      "PCA fitted with 30 components, explaining 0.7887 variance\n",
      "✓ OVO model trained for pair (0, 3) | Samples: 2009\n",
      "PCA fitted with 30 components, explaining 0.7946 variance\n",
      "✓ OVO model trained for pair (0, 4) | Samples: 1961\n",
      "PCA fitted with 30 components, explaining 0.7819 variance\n",
      "✓ OVO model trained for pair (0, 5) | Samples: 1891\n",
      "PCA fitted with 30 components, explaining 0.8102 variance\n",
      "✓ OVO model trained for pair (0, 6) | Samples: 1974\n",
      "PCA fitted with 30 components, explaining 0.8044 variance\n",
      "✓ OVO model trained for pair (0, 7) | Samples: 2031\n",
      "PCA fitted with 30 components, explaining 0.7756 variance\n",
      "✓ OVO model trained for pair (0, 8) | Samples: 1962\n",
      "PCA fitted with 30 components, explaining 0.8058 variance\n",
      "✓ OVO model trained for pair (0, 9) | Samples: 1979\n",
      "PCA fitted with 30 components, explaining 0.7924 variance\n",
      "✓ OVO model trained for pair (1, 2) | Samples: 2117\n",
      "PCA fitted with 30 components, explaining 0.7979 variance\n",
      "✓ OVO model trained for pair (1, 3) | Samples: 2146\n",
      "PCA fitted with 30 components, explaining 0.8294 variance\n",
      "✓ OVO model trained for pair (1, 4) | Samples: 2098\n",
      "PCA fitted with 30 components, explaining 0.7977 variance\n",
      "✓ OVO model trained for pair (1, 5) | Samples: 2028\n",
      "PCA fitted with 30 components, explaining 0.8337 variance\n",
      "✓ OVO model trained for pair (1, 6) | Samples: 2111\n",
      "PCA fitted with 30 components, explaining 0.8300 variance\n",
      "✓ OVO model trained for pair (1, 7) | Samples: 2168\n",
      "PCA fitted with 30 components, explaining 0.7955 variance\n",
      "✓ OVO model trained for pair (1, 8) | Samples: 2099\n",
      "PCA fitted with 30 components, explaining 0.8308 variance\n",
      "✓ OVO model trained for pair (1, 9) | Samples: 2116\n",
      "PCA fitted with 30 components, explaining 0.7462 variance\n",
      "✓ OVO model trained for pair (2, 3) | Samples: 2015\n",
      "PCA fitted with 30 components, explaining 0.7462 variance\n",
      "✓ OVO model trained for pair (2, 4) | Samples: 1967\n",
      "PCA fitted with 30 components, explaining 0.7349 variance\n",
      "✓ OVO model trained for pair (2, 5) | Samples: 1897\n",
      "PCA fitted with 30 components, explaining 0.7431 variance\n",
      "✓ OVO model trained for pair (2, 6) | Samples: 1980\n",
      "PCA fitted with 30 components, explaining 0.7602 variance\n",
      "✓ OVO model trained for pair (2, 7) | Samples: 2037\n",
      "PCA fitted with 30 components, explaining 0.7223 variance\n",
      "✓ OVO model trained for pair (2, 8) | Samples: 1968\n",
      "PCA fitted with 30 components, explaining 0.7535 variance\n",
      "✓ OVO model trained for pair (2, 9) | Samples: 1985\n",
      "PCA fitted with 30 components, explaining 0.7545 variance\n",
      "✓ OVO model trained for pair (3, 4) | Samples: 1996\n",
      "PCA fitted with 30 components, explaining 0.7527 variance\n",
      "✓ OVO model trained for pair (3, 5) | Samples: 1926\n",
      "PCA fitted with 30 components, explaining 0.7680 variance\n",
      "✓ OVO model trained for pair (3, 6) | Samples: 2009\n",
      "PCA fitted with 30 components, explaining 0.7641 variance\n",
      "✓ OVO model trained for pair (3, 7) | Samples: 2066\n",
      "PCA fitted with 30 components, explaining 0.7354 variance\n",
      "✓ OVO model trained for pair (3, 8) | Samples: 1997\n",
      "PCA fitted with 30 components, explaining 0.7633 variance\n",
      "✓ OVO model trained for pair (3, 9) | Samples: 2014\n",
      "PCA fitted with 30 components, explaining 0.7481 variance\n",
      "✓ OVO model trained for pair (4, 5) | Samples: 1878\n",
      "PCA fitted with 30 components, explaining 0.7664 variance\n",
      "✓ OVO model trained for pair (4, 6) | Samples: 1961\n",
      "PCA fitted with 30 components, explaining 0.7792 variance\n",
      "✓ OVO model trained for pair (4, 7) | Samples: 2018\n",
      "PCA fitted with 30 components, explaining 0.7415 variance\n",
      "✓ OVO model trained for pair (4, 8) | Samples: 1949\n",
      "PCA fitted with 30 components, explaining 0.7749 variance\n",
      "✓ OVO model trained for pair (4, 9) | Samples: 1966\n",
      "PCA fitted with 30 components, explaining 0.7684 variance\n",
      "✓ OVO model trained for pair (5, 6) | Samples: 1891\n",
      "PCA fitted with 30 components, explaining 0.7586 variance\n",
      "✓ OVO model trained for pair (5, 7) | Samples: 1948\n",
      "PCA fitted with 30 components, explaining 0.7365 variance\n",
      "✓ OVO model trained for pair (5, 8) | Samples: 1879\n",
      "PCA fitted with 30 components, explaining 0.7585 variance\n",
      "✓ OVO model trained for pair (5, 9) | Samples: 1896\n",
      "PCA fitted with 30 components, explaining 0.7799 variance\n",
      "✓ OVO model trained for pair (6, 7) | Samples: 2031\n",
      "PCA fitted with 30 components, explaining 0.7496 variance\n",
      "✓ OVO model trained for pair (6, 8) | Samples: 1962\n",
      "PCA fitted with 30 components, explaining 0.7754 variance\n",
      "✓ OVO model trained for pair (6, 9) | Samples: 1979\n",
      "PCA fitted with 30 components, explaining 0.7524 variance\n",
      "✓ OVO model trained for pair (7, 8) | Samples: 2019\n",
      "PCA fitted with 30 components, explaining 0.7942 variance\n",
      "✓ OVO model trained for pair (7, 9) | Samples: 2036\n",
      "PCA fitted with 30 components, explaining 0.7472 variance\n",
      "✓ OVO model trained for pair (8, 9) | Samples: 1967\n",
      "\n",
      "All 45 OVO KNN models trained successfully.\n",
      "\n",
      "================= OVO-KNN PREDICTION =================\n",
      "Predicted 0/2499\n",
      "Predicted 200/2499\n",
      "Predicted 400/2499\n",
      "Predicted 600/2499\n",
      "Predicted 800/2499\n",
      "Predicted 1000/2499\n",
      "Predicted 1200/2499\n",
      "Predicted 1400/2499\n",
      "Predicted 1600/2499\n",
      "Predicted 1800/2499\n",
      "Predicted 2000/2499\n",
      "Predicted 2200/2499\n",
      "Predicted 2400/2499\n",
      "\n",
      "================= FINAL OVO-KNN RESULTS =================\n",
      "Accuracy: 0.9503801520608244\n",
      "[[242   0   0   0   0   0   3   1   0   1]\n",
      " [  0 279   1   0   0   0   0   1   0   0]\n",
      " [  0   1 238   1   0   0   0   5   3   0]\n",
      " [  0   2   3 237   0   3   0   5   4   1]\n",
      " [  1   0   0   0 231   0   0   2   0   9]\n",
      " [  0   0   1   6   0 214   3   0   1   1]\n",
      " [  4   0   0   1   0   3 238   0   0   0]\n",
      " [  0   2   1   1   1   0   0 254   0   2]\n",
      " [  1   3   3   3   1   6   2   2 218   5]\n",
      " [  0   0   0   3  14   1   0   6   0 224]]\n"
     ]
    }
   ],
   "source": [
    "y_pred, ovo_knn, ovo_pca = run_ovo_knn_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aafc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ovr_knn_models(Xtrain, ytrain):\n",
    "    \"\"\"\n",
    "    Train 10 One-Vs-Rest PCA + KNN(k=1) models.\n",
    "    \n",
    "    Returns:\n",
    "        ovr_knn[d] = KNN for digit d vs rest\n",
    "        ovr_pca[d] = PCA for digit d vs rest\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n================= TRAINING 10 OVR KNN MODELS =================\")\n",
    "\n",
    "    ovr_knn = {}\n",
    "    ovr_pca = {}\n",
    "\n",
    "    digits = list(range(10))\n",
    "\n",
    "    for d in digits:\n",
    "\n",
    "        # Mask: digit = d (positive) or not d (negative)\n",
    "        mask = (ytrain == d)\n",
    "        X_pos = Xtrain[mask]\n",
    "        X_neg = Xtrain[~mask]\n",
    "\n",
    "        # Build OVR dataset: keep all positives + a sampled set of negatives\n",
    "        # (optional: downsample negatives to speed up)\n",
    "        # X_comb = np.vstack([X_pos, X_neg])\n",
    "        # y_comb = np.concatenate([np.ones(len(X_pos)), np.zeros(len(X_neg))])\n",
    "\n",
    "        X_comb = Xtrain  # FULL dataset (best accuracy)\n",
    "        y_comb = (ytrain == d).astype(int)\n",
    "\n",
    "        # PCA for this class\n",
    "        pca_d = PCA(n_components=30)\n",
    "        X_pca = pca_d.fit_transform(X_comb)\n",
    "\n",
    "        # Train KNN\n",
    "        knn_d = KNN(k=1)\n",
    "        knn_d.fit(X_pca, y_comb)\n",
    "\n",
    "        ovr_knn[d] = knn_d\n",
    "        ovr_pca[d] = pca_d\n",
    "\n",
    "        print(f\"✓ OVR model trained for digit {d} | Positives={len(X_pos)}\")\n",
    "\n",
    "    print(\"\\nAll 10 OVR KNN models trained successfully.\")\n",
    "    return ovr_knn, ovr_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3150863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ovr_knn_predict(ovr_knn, ovr_pca, X):\n",
    "    \"\"\"\n",
    "    Predict digits using 10 One-Vs-Rest KNN models.\n",
    "    Works for VAL or TEST.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    y_pred = np.zeros(n, dtype=int)\n",
    "\n",
    "    print(\"\\n================= OVR-KNN PREDICTION =================\")\n",
    "\n",
    "    for idx in range(n):\n",
    "\n",
    "        x = X.iloc[idx:idx+1]\n",
    "        scores = {}\n",
    "\n",
    "        for d in range(10):\n",
    "            pca_d = ovr_pca[d]\n",
    "            knn_d = ovr_knn[d]\n",
    "\n",
    "            # PCA transform\n",
    "            x_pca = pca_d.transform(x)\n",
    "\n",
    "            # 1 = positive class → used as confidence\n",
    "            pred = knn_d.predict(x_pca)[0]\n",
    "            scores[d] = pred\n",
    "\n",
    "        # pick max-score digit\n",
    "        y_pred[idx] = max(scores, key=scores.get)\n",
    "\n",
    "        if idx % 200 == 0:\n",
    "            print(f\"Predicted {idx}/{n}\")\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8165582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ovr_knn_pipeline(train_csv='MNIST_train.csv',\n",
    "                         val_csv='MNIST_validation.csv',\n",
    "                         test_csv='MNIST_test.csv'):\n",
    "\n",
    "    # --- Load data ---\n",
    "    (Xtrain, Xval, Xtest,\n",
    "     ytrain, yval, ytest,\n",
    "     ytrain2, yval2) = read_data(train_csv, val_csv, test_csv)\n",
    "\n",
    "    # --- Train OVR ---\n",
    "    ovr_knn, ovr_pca = train_ovr_knn_models(Xtrain, ytrain)\n",
    "\n",
    "    # --- Predict VAL ---\n",
    "    y_pred_val = ovr_knn_predict(ovr_knn, ovr_pca, Xval)\n",
    "\n",
    "    print(\"\\n================= VALIDATION RESULTS =================\")\n",
    "    print(\"Accuracy:\", accuracy_score(yval, y_pred_val))\n",
    "    print(confusion_matrix(yval, y_pred_val))\n",
    "\n",
    "    # --- Predict TEST ---\n",
    "    y_pred_test = ovr_knn_predict(ovr_knn, ovr_pca, Xtest)\n",
    "\n",
    "    print(\"\\n================= TEST RESULTS =================\")\n",
    "    print(\"Accuracy:\", accuracy_score(ytest, y_pred_test))\n",
    "    print(confusion_matrix(ytest, y_pred_test))\n",
    "\n",
    "    return y_pred_val, y_pred_test, ovr_knn, ovr_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362da5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= TRAINING 10 OVR KNN MODELS =================\n",
      "PCA fitted with 30 components, explaining 0.7306 variance\n",
      "✓ OVR model trained for digit 0 | Positives=987\n",
      "PCA fitted with 30 components, explaining 0.7306 variance\n",
      "✓ OVR model trained for digit 1 | Positives=1124\n",
      "PCA fitted with 30 components, explaining 0.7306 variance\n",
      "✓ OVR model trained for digit 2 | Positives=993\n",
      "PCA fitted with 30 components, explaining 0.7306 variance\n",
      "✓ OVR model trained for digit 3 | Positives=1022\n",
      "PCA fitted with 30 components, explaining 0.7306 variance\n",
      "✓ OVR model trained for digit 4 | Positives=974\n",
      "PCA fitted with 30 components, explaining 0.7306 variance\n",
      "✓ OVR model trained for digit 5 | Positives=904\n",
      "PCA fitted with 30 components, explaining 0.7306 variance\n",
      "✓ OVR model trained for digit 6 | Positives=987\n",
      "PCA fitted with 30 components, explaining 0.7306 variance\n",
      "✓ OVR model trained for digit 7 | Positives=1044\n",
      "PCA fitted with 30 components, explaining 0.7306 variance\n",
      "✓ OVR model trained for digit 8 | Positives=975\n",
      "PCA fitted with 30 components, explaining 0.7306 variance\n",
      "✓ OVR model trained for digit 9 | Positives=992\n",
      "\n",
      "All 10 OVR KNN models trained successfully.\n",
      "\n",
      "================= OVR-KNN PREDICTION =================\n",
      "Predicted 0/2499\n",
      "Predicted 200/2499\n",
      "Predicted 400/2499\n",
      "Predicted 600/2499\n",
      "Predicted 800/2499\n",
      "Predicted 1000/2499\n",
      "Predicted 1200/2499\n"
     ]
    }
   ],
   "source": [
    "run_ovr_knn_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
